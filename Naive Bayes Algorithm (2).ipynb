{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f4c964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given tweets are:\n",
      "['This cinema is one of the best film i have seen in my life', 'It is a average game.', 'Apple is a good product in the market']\n",
      "Tweet: This cinema is one of the best film i have seen in my life\n",
      "Domain Label: cinema\n",
      "Sentiment Label: positive\n",
      "\n",
      "Tweet: It is a average game.\n",
      "Domain Label: sports\n",
      "Sentiment Label: negative\n",
      "\n",
      "Tweet: Apple is a good product in the market\n",
      "Domain Label: technology\n",
      "Sentiment Label: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load your labeled domain tweet data\n",
    "data_domain = pd.read_csv('C:/Users/dell/OneDrive/Desktop/dataset/Book2.csv')\n",
    "\n",
    "# Load your labeled sentiment tweet data\n",
    "data_sentiment = pd.read_csv('C:/Users/dell/OneDrive/Desktop/dataset/twitter_training.csv')\n",
    "\n",
    "# Data preprocessing for domain classification\n",
    "def preprocess_domain_data(data):\n",
    "    # Remove any rows with missing data\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Encode the domain labels\n",
    "    le_domain = LabelEncoder()\n",
    "    data['domain_label'] = le_domain.fit_transform(data['Category'])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Data preprocessing for sentiment classification\n",
    "def preprocess_sentiment_data(data):\n",
    "    # Remove any rows with missing data\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Encode the sentiment labels\n",
    "    le_sentiment = LabelEncoder()\n",
    "    data['sentiment_label'] = le_sentiment.fit_transform(data['Sentiment'])\n",
    "\n",
    "    return data \n",
    "\n",
    "# Preprocess the domain data\n",
    "data_domain = preprocess_domain_data(data_domain)\n",
    "\n",
    "# Split the domain data into training and testing sets\n",
    "X_domain = data_domain['Tweets'].astype(str)\n",
    "y_domain = data_domain['domain_label']\n",
    "\n",
    "X_train_domain, X_test_domain, y_train_domain, y_test_domain = train_test_split(X_domain, y_domain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text using CountVectorizer for domain classification\n",
    "vectorizer_domain = CountVectorizer()\n",
    "X_train_counts_domain = vectorizer_domain.fit_transform(X_train_domain)\n",
    "X_test_counts_domain = vectorizer_domain.transform(X_test_domain)\n",
    "\n",
    "# Preprocess the text for domain classification\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing Punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenization (splitting by space)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Define your own list of stopwords\n",
    "    stop_words = set(['the', 'and', 'is', 'in', 'it', 'of', 'for', 'this', 'to', 'a'])\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join the cleaned tokens back into a text string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Create and train the Naive Bayes classifier for domain classification\n",
    "nb_classifier_domain = MultinomialNB()\n",
    "nb_classifier_domain.fit(X_train_counts_domain, y_train_domain)\n",
    "\n",
    "# Preprocess the sentiment data\n",
    "data_sentiment = preprocess_sentiment_data(data_sentiment)\n",
    "\n",
    "# Split the sentiment data into training and testing sets\n",
    "X_sentiment = data_sentiment['Tweets'].astype(str)\n",
    "y_sentiment = data_sentiment['sentiment_label']\n",
    "\n",
    "X_train_sentiment, X_test_sentiment, y_train_sentiment, y_test_sentiment = train_test_split(X_sentiment, y_sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text using CountVectorizer for sentiment classification\n",
    "vectorizer_sentiment = CountVectorizer()\n",
    "X_train_counts_sentiment = vectorizer_sentiment.fit_transform(X_train_sentiment)\n",
    "X_test_counts_sentiment = vectorizer_sentiment.transform(X_test_sentiment)\n",
    "\n",
    "# Create and train the Naive Bayes classifier for sentiment classification\n",
    "nb_classifier_sentiment = MultinomialNB()\n",
    "nb_classifier_sentiment.fit(X_train_counts_sentiment, y_train_sentiment)\n",
    "\n",
    "# Example new tweets\n",
    "new_tweets = [\n",
    "    \"This cinema is one of the best film i have seen in my life\",\n",
    "    \"It is a average game.\",\n",
    "    \"Apple is a good product in the market\"\n",
    "]\n",
    "\n",
    "# Input sentiment labels for the new tweets (obtained from the user)\n",
    "user_provided_sentiments = ['positive', 'negative', 'positive']\n",
    "\n",
    "# Make predictions using the trained classifier for domain classification\n",
    "new_tweets_counts_domain = vectorizer_domain.transform(new_tweets)\n",
    "new_tweet_predictions_domain = nb_classifier_domain.predict(new_tweets_counts_domain)\n",
    "\n",
    "# Map the numeric predictions to domain labels\n",
    "label_mapping_domain = {1: 'cinema', 2: 'sports', 4: 'technology'}  # Replace with actual labels\n",
    "predicted_labels_domain = [label_mapping_domain.get(prediction, 'unknown') for prediction in new_tweet_predictions_domain]\n",
    "\n",
    "# Map user-provided sentiment labels to numeric labels\n",
    "label_mapping_sentiment = {'negative': 0, 'neutral': 1, 'positive': 2} \n",
    "\n",
    "# Make predictions using the trained classifier for sentiment classification\n",
    "new_tweets_counts_sentiment = vectorizer_sentiment.transform(new_tweets)\n",
    "new_tweet_predictions_sentiment = [label_mapping_sentiment[sentiment] for sentiment in user_provided_sentiments]\n",
    "\n",
    "# Print the predictions with domain and sentiment labels\n",
    "print(\"The given tweets are:\")\n",
    "print(new_tweets)\n",
    "for tweet, domain_label, sentiment_label in zip(new_tweets, predicted_labels_domain, user_provided_sentiments):\n",
    "    print(f\"Tweet: {tweet}\\nDomain Label: {domain_label}\\nSentiment Label: {sentiment_label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec97042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3975d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabfb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db1d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c1843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62286eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee811d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c64a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf0b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ea462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c28512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a442de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925119e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d83c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a97c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a15a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e68f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541daa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389c4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35f84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac7c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classification Metrics:\n",
      "Accuracy: 0.9401709401709402\n",
      "Precision: 0.9409377173849387\n",
      "Recall: 0.9401709401709402\n",
      "F1 Score: 0.9402412521826622\n",
      "\n",
      "Classification Report for Domain Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cinema       0.97      0.92      0.94        90\n",
      "      sports       0.91      0.95      0.93        78\n",
      "  technology       0.94      0.95      0.95        66\n",
      "\n",
      "    accuracy                           0.94       234\n",
      "   macro avg       0.94      0.94      0.94       234\n",
      "weighted avg       0.94      0.94      0.94       234\n",
      "\n",
      "\n",
      "Sentiment Classification Metrics:\n",
      "Accuracy: 0.776268666801811\n",
      "Precision: 0.7810313042317172\n",
      "Recall: 0.776268666801811\n",
      "F1 Score: 0.7762236319843844\n",
      "\n",
      "Classification Report for Sentiment Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.82      0.78      4463\n",
      "     neutral       0.83      0.73      0.78      6213\n",
      "    positive       0.75      0.80      0.77      4123\n",
      "\n",
      "    accuracy                           0.78     14799\n",
      "   macro avg       0.77      0.78      0.78     14799\n",
      "weighted avg       0.78      0.78      0.78     14799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Domain classification performance metrics\n",
    "y_pred_domain = nb_classifier_domain.predict(X_test_counts_domain)\n",
    "accuracy_domain = accuracy_score(y_test_domain, y_pred_domain)\n",
    "precision_domain = precision_score(y_test_domain, y_pred_domain, average='weighted')\n",
    "recall_domain = recall_score(y_test_domain, y_pred_domain, average='weighted')\n",
    "f1_score_domain = f1_score(y_test_domain, y_pred_domain, average='weighted')\n",
    "\n",
    "# Generate target names based on unique class labels in y_test_domain\n",
    "target_names_domain = [label_mapping_domain[i] for i in np.unique(y_test_domain)]\n",
    "\n",
    "# Generate a classification report for domain classification\n",
    "report_domain = classification_report(y_test_domain, y_pred_domain, labels=np.unique(y_test_domain), target_names=target_names_domain)\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment classification performance metrics\n",
    "y_pred_sentiment = nb_classifier_sentiment.predict(X_test_counts_sentiment)\n",
    "accuracy_sentiment = accuracy_score(y_test_sentiment, y_pred_sentiment)\n",
    "precision_sentiment = precision_score(y_test_sentiment, y_pred_sentiment, average='weighted')\n",
    "recall_sentiment = recall_score(y_test_sentiment, y_pred_sentiment, average='weighted')\n",
    "f1_score_sentiment = f1_score(y_test_sentiment, y_pred_sentiment, average='weighted')\n",
    "\n",
    "# Generate target names based on label_mapping_sentiment\n",
    "target_names_sentiment = list(label_mapping_sentiment.keys())\n",
    "\n",
    "\n",
    "# Generate a classification report for sentiment classification\n",
    "report_sentiment = classification_report(y_test_sentiment, y_pred_sentiment, labels=np.unique(y_test_sentiment), target_names=target_names_sentiment)\n",
    "\n",
    "\n",
    "# Print performance metrics for domain classification\n",
    "print(\"Domain Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_domain}\")\n",
    "print(f\"Precision: {precision_domain}\")\n",
    "print(f\"Recall: {recall_domain}\")\n",
    "print(f\"F1 Score: {f1_score_domain}\")\n",
    "print(\"\\nClassification Report for Domain Classification:\")\n",
    "print(report_domain)\n",
    "\n",
    "# Print performance metrics for sentiment classification\n",
    "print(\"\\nSentiment Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_sentiment}\")\n",
    "print(f\"Precision: {precision_sentiment}\")\n",
    "print(f\"Recall: {recall_sentiment}\")\n",
    "print(f\"F1 Score: {f1_score_sentiment}\")\n",
    "print(\"\\nClassification Report for Sentiment Classification:\")\n",
    "print(report_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c417a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496d72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec880d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47219d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a25ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc934bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67107806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f9835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6fa06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69524e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7587d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e1d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2909bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00514855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1c7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621722b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f54eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f836be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98fef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04332c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf9a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395dffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2b1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fc5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a42fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfffbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559d8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a2a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc37e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486539ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83df5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566146a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935cb8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a8347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d77f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aec0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fad38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab97fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3a892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b96188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30b23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6caa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb2fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da63200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63b25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b72c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b77b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874bf038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea08c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c4389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
